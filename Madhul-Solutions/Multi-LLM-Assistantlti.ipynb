{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ollama\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "OLLAMA_API = \"http://192.168.0.207:11434/v1\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "OLLAMA_API_KEY = 'ollama'\n",
    "#Define LLM models\n",
    "MODEL_OPEN_API = 'gpt-4o-mini'\n",
    "MODEL_OLLAMA = 'llama3.2'\n",
    "openai = OpenAI()\n",
    "question=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up OpenAI environment\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd9fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a personal AI assistant. The following is a an example conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\\n\\nHuman: Hello, who are you?\\nAI: I am an AI assistant. How can I help you today?\\nHuman: ... Respond in Markdown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\\n \"+ input(\"Please enter your Question :\")\n",
    "#question = \"\"\"\n",
    "#Please explain what this code does and why:\n",
    "#yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f95067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question():\n",
    "    global question\n",
    "    question = input(\"Please enter your Question :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f744fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "def get_messages():\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ccf39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_response(text):\n",
    "    display(Markdown(text))\n",
    "    #update_display(Markdown(text), display_id='response', display_id_2='response')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bcc31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(response):\n",
    "    response_text = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in response:\n",
    "        response_text += chunk.choices[0].delta.content or ''\n",
    "        response_text = response_text.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response_text), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6ef4f",
   "metadata": {},
   "source": [
    "#<blockquote>\n",
    "ask_question()\n",
    "# RESPONSE FROM OLLAMA MODEL\n",
    "##Create Ollama_API object \n",
    "ollama_via_openai = OpenAI(base_url=OLLAMA_API, api_key=OLLAMA_API_KEY)\n",
    "\n",
    "#response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "response = ollama_via_openai.chat.completions.create(model=MODEL_OLLAMA, messages=get_messages(),stream=True)\n",
    "\n",
    "display_response(f\"# **Response from OLLAMA model: {MODEL_OLLAMA.capitalize()}** \\n\")\n",
    "\n",
    "##### Uncomment the line below to stream content\n",
    "stream_response(response)\n",
    "\n",
    "##### Uncomment the line below to stream content\n",
    "#display_response(response.choices[0].message.content)\n",
    "display(Markdown(\"#### **--- End of response from Ollama ---**\"))  \n",
    "#</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea226ecf",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "# RESPONSE FROM OPEN API MODEL\n",
    "##Assing OpenAI API Key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "response = openai.chat.completions.create(model=MODEL_OPEN_API, messages=get_messages(),stream=True)\n",
    "\n",
    "display_response(f\"# **Response from OpenAPI\\'s model: {MODEL_OPEN_API.capitalize()}** \\n\")\n",
    "\n",
    "##### Uncomment the line below to stream content\n",
    "stream_response(response)\n",
    "\n",
    "##### Uncomment the line below to stream content\n",
    "#display_response(response.choices[0].message.content)\n",
    "\n",
    "display(Markdown(\"#### **--- End of response from OpenAPI ---**\")) \n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84fece9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# **Response from OLLAMA model: Llama3.2** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Transformers for Large Language Models (LLMs)**\n",
       "=====================================================\n",
       "\n",
       "As a beginner, it's essential to understand the basics of how Transformers work and their application in Large Language Models (LLMs).\n",
       "\n",
       "### **Traditional Neural Networks**\n",
       "\n",
       "Until 2017, Recurrent Neural Networks (RNNs) were widely used for natural language processing (NLP) tasks. RNNs process sequences one step at a time, making them well-suited for NLP. However, they have limitations when dealing with longer sequences or parallelization.\n",
       "\n",
       "### **Transformer Architecture**\n",
       "\n",
       "The Transformer architecture was introduced in 2017 by Vaswani et al. and revolutionized the field of NLP. It's an attention-based neural network that replaces traditional RNNs.\n",
       "\n",
       "**Key Components:**\n",
       "\n",
       "*   **Self-Attention Mechanism:** This allows the model to weigh the importance of different input elements relative to each other. The self-attention mechanism is a key component of the transformer architecture.\n",
       "*   **Feed Forward Network (FFN):** An FFN consists of two linear layers with a ReLU activation function in between. The FFN is used for encoding sequential information.\n",
       "*   **Multi-Head Attention:** This allows multiple attention mechanisms to be simultaneously computed and combined, increasing the model's capacity.\n",
       "\n",
       "### **How Transformers Work**\n",
       "\n",
       "Here's an overview of how transformers process sequences:\n",
       "\n",
       "1.  **Input Embeddings:** The input sequence is embedded into a high-dimensional space using pre-trained embeddings like WordPiece or BERT-based embeddings.\n",
       "2.  **Positional Encoding:** A positional encoding is added to the input sequence to preserve the order information.\n",
       "3.  **Self-Attention Mechanism:** At each position in the sequence, the self-attention mechanism computes the attention scores based on the input elements at that position and all other positions.\n",
       "4.  **FFN:** After self-attention, an FFN is applied to compress sequential information.\n",
       "5.  **Feed Forward Network Module:** The FFN is followed by another linear layer with a ReLU activation function.\n",
       "\n",
       "### **Large Language Models (LLMs)**\n",
       "\n",
       "LLMs leverage transformer architecture in combination with pre-trained language representations. This combines the strengths of:\n",
       "\n",
       "*   **Pre-trained Large Language Models** (like BERT or RoBERTa): Provide initial word representation and contextual information.\n",
       "*   **Transformers:** Enable parallel processing, improving the efficiency and efficiency of LLMs.\n",
       "\n",
       "### Benefits\n",
       "\n",
       "The adoption of transformers in large language models offers several benefits:\n",
       "\n",
       "*   **Improved Parallelization:** Transformers can be parallelized efficiently.\n",
       "*   **Long-Range Dependency Representation:** The Transformer's self-attention mechanism can capture long-range dependencies effectively.\n",
       "*   **Increased Contextual Understanding:** Using a larger pre-trained model and adding transformer layers enhances the contextual understanding and vocabulary coverage of LLMs.\n",
       "\n",
       "### Example Code\n",
       "\n",
       "Let's demonstrate how to implement the basic Transformer architecture using Python and PyTorch.\n",
       "\n",
       "python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "\n",
       "class Transformer(nn.Module):\n",
       "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_heads, dropout):\n",
       "        super(Transformer, self).__init__()\n",
       "\n",
       "        # Input embedding layer\n",
       "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
       "\n",
       "        # Self-attention mechanism\n",
       "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
       "        self.query_layer = Linear(embed_dim, hidden_dim)\n",
       "        self.key_layer = Linear(embed_dim, hidden_dim)\n",
       "        self.value_layer = Linear(embed_dim, hidden_dim)\n",
       "\n",
       "        # FFN layer\n",
       "        self.ffn = FeedForwardNetwork(hidden_dim)\n",
       "\n",
       "    def forward(self, x):\n",
       "        # Positional encoding\n",
       "        pos_encoding = positional_encode(x.size(1), x=256)\n",
       "\n",
       "        # Input embedding\n",
       "        embedded_input = self.embedding(x) + pos_encoding\n",
       "\n",
       "        # Self-attention mechanism\n",
       "        attention_output, _ = self.self_attn(embedded_input)\n",
       "        output = self.query_layer(attention_output)\n",
       "        output = self.key_layer(output)\n",
       "        output = self.value_layer(output)\n",
       "\n",
       "        # FFN layer\n",
       "        ffn_output = self.ffn(output)\n",
       "        return ffn_output\n",
       "\n",
       "class MultiHeadAttention(nn.Module):\n",
       "    def __init__(self, embed_dim, num_heads, dropout):\n",
       "        super(MultiHeadAttention, self).__init__()\n",
       "\n",
       "        self.heads = nn.ModuleList([DenseHead(embed_dim, num_heads) for _ in range(num_heads)])\n",
       "\n",
       "        self.dropout = nn.Dropout(dropout)\n",
       "\n",
       "    def forward(self, x):\n",
       "        attention_output, weights = self.apply_multi_head_attention(x)\n",
       "        return torch.matmul(attention_output, weights)\n",
       "\n",
       "class DenseHead(nn.Module):\n",
       "    def __init__(self, embed_dim, num_heads):\n",
       "        super(DenseHead, self).__init__()\n",
       "\n",
       "        self.weight layer = nn.Linear(embed_dim, embed_dim)\n",
       "        self.query_layer = nn.Parameter(torch.randn(num_heads * 1, embed_dim))\n",
       "        self.key_layer = nn.Parameter(torch.randn(num_heads * 1, embed_dim))\n",
       "\n",
       "    def forward(self, x):\n",
       "        query = torch.matmul(x, self.query_layer)\n",
       "        key = torch.matmul(x, self.key_layer)\n",
       "\n",
       "        attention_weights = torch.matmul(query, key.T) / math.sqrt(embed_dim)\n",
       "        output, weights = self.drop_attention(attention_weights)\n",
       "\n",
       "        return output\n",
       "\n",
       "\n",
       "\n",
       "This implementation provides an efficient and effective way to understand the fundamental Transformer-based model's components, architecture, and applications in Large Language Models by focusing on pre-trained models as initial representations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### **--- End of response from Ollama ---**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################################################\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# **Response from OpenAPI's model: Gpt-4o-mini** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Sure! Let’s break down the concept of transformers, especially in the context of large language models (LLMs) in a simple, beginner-friendly way.\n",
       "\n",
       "## What is a Transformer?\n",
       "\n",
       "A **transformer** is a type of model architecture used primarily in natural language processing (NLP). It was introduced in a paper called \"Attention is All You Need\" by Vaswani et al. in 2017. What makes transformers special is their ability to handle sequences of data (like sentences) more effectively than previous models.\n",
       "\n",
       "### Key Features of Transformers:\n",
       "\n",
       "1. **Attention Mechanism**:\n",
       "   - The core idea of a transformer is the **attention mechanism**. This allows the model to focus on different parts of the input sequence when processing it. If you're reading a sentence, for example, you might need to pay more attention to certain words to understand the context.\n",
       "   - There are different types of attention within a transformer, but the most commonly used is **self-attention**, which helps a word consider other words in the same sentence.\n",
       "\n",
       "2. **Parallel Processing**:\n",
       "   - Unlike older models like RNN (Recurrent Neural Networks), transformers process all words in a sentence simultaneously, rather than one at a time. This makes them much faster and allows them to train on large datasets effectively.\n",
       "\n",
       "3. **Layers**:\n",
       "   - A transformer is composed of multiple layers. Each layer applies attention and feeds the information to the next layer. This stacking of layers helps the model learn complex patterns in the data.\n",
       "\n",
       "4. **Positional Encoding**:\n",
       "   - Since transformers don't inherently understand the order of words, they use **positional encoding** to add information about the position of each word in the sequence. This helps maintain the structure of the sentence.\n",
       "\n",
       "## Transformers and Large Language Models (LLMs)\n",
       "\n",
       "**Large Language Models (LLMs)**, like GPT-3 (and GPT-4), are built using transformer architecture. Here’s how it works:\n",
       "\n",
       "1. **Training on Large Datasets**:\n",
       "   - LLMs are trained on a vast amount of text data. They learn to predict the next word in a sentence, based on the context of the words before it. This training makes them good at understanding language patterns.\n",
       "\n",
       "2. **Generating Text**:\n",
       "   - Once trained, LLMs can generate coherent and contextually relevant text based on the prompts they receive. For example, if you ask it a question or give it a sentence to complete, it uses the patterns it learned to generate a response.\n",
       "\n",
       "3. **Fine-Tuning and Versatility**:\n",
       "   - LLMs can be fine-tuned for specific tasks, such as translation, summarization, or answering questions based on particular fields. This adaptability is one of the powerful aspects of transformers.\n",
       "\n",
       "### Visualizing Transformers\n",
       "\n",
       "To visualize a transformer, think of it as a multi-layered system where information moves between layers, with each layer being able to pay attention to different parts of the input sequence. Here's a simple schematic representation:\n",
       "\n",
       "\n",
       "Input Sequence ---> [ Layer 1 (Attention & Processing) ]\n",
       "                          |\n",
       "                          v\n",
       "                    [ Layer 2 (Attention & Processing) ]\n",
       "                          |\n",
       "                          v\n",
       "                    ...\n",
       "                          |\n",
       "                          v\n",
       "                   [ Output Sequence ]\n",
       "\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In summary, transformers are a revolutionary model architecture, particularly effective for language processing tasks. Their ability to understand context through the attention mechanism, combined with parallel processing and scalability, makes them the backbone of many advanced language models today. \n",
       "\n",
       "If you have any specific questions or topics you'd like to explore further, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### **--- End of response from OpenAPI ---**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Ask Question and assign to question variable\n",
    "ask_question()\n",
    "\n",
    "# RESPONSE FROM OLLAMA MODEL\n",
    "##Create Ollama_API object \n",
    "ollama_via_openai = OpenAI(base_url=OLLAMA_API, api_key=OLLAMA_API_KEY)\n",
    "\n",
    "#response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "response = ollama_via_openai.chat.completions.create(model=MODEL_OLLAMA, messages=get_messages(),stream=True)\n",
    "\n",
    "display_response(f\"# **Response from OLLAMA model: {MODEL_OLLAMA.capitalize()}** \\n\")\n",
    "\n",
    "### Uncomment the line below to stream content\n",
    "stream_response(response)\n",
    "\n",
    "### Uncomment the line below to stream content\n",
    "#display_response(response.choices[0].message.content)\n",
    "display(Markdown(\"#### **--- End of response from Ollama ---**\"))  \n",
    "\n",
    "print(\"###########################################################################\")\n",
    "\n",
    "#RESPONSE FROM OPEN API MODEL\n",
    "##Assing OpenAI API Key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "response = openai.chat.completions.create(model=MODEL_OPEN_API, messages=get_messages(),stream=True)\n",
    "\n",
    "display_response(f\"# **Response from OpenAPI\\'s model: {MODEL_OPEN_API.capitalize()}** \\n\")\n",
    "\n",
    "### Uncomment the line below to stream content\n",
    "stream_response(response)\n",
    "\n",
    "### Uncomment the line below to stream content\n",
    "#display_response(response.choices[0].message.content)\n",
    "\n",
    "display(Markdown(\"#### **--- End of response from OpenAPI ---**\")) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
